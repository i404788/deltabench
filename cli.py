import os 
import tempfile
import filecmp
from collections import defaultdict
from time import time
from tqdm import tqdm
from glob import glob
from cleo import Command, Application

from odess import SimilarityIndex

# TODO: add bench pair-list command (auto-detect gdelta, xdelta cli?)
# TODO: pop file, find_similar, use largest as base others as targets
# TODO: calc total size of documents in-place
def argmax(d: dict):
    return max(d, key=d.get)

def intersect_dict(d: dict, l: list):
    return {k: v for k,v in d.items() if k in l}

class TimeToDict:
    def __init__(self, d, key):
        self.d = d
        self.key = key

    def __enter__(self):
        self.stime = time()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.d[k] += time() - self.stime


class BenchmarkPairsCommand(Command):
    """
    Calculates the initial & compressed size of a pair list (generated by generate_pairs) using different algorithms. Algorithms need to be compatible CLIs, see xdelta.py for example
    
    bench
        {pair_file : Pair file generated with generate_pairs}
        {algorithms?* : Algorithm to try, should be in PATH with a compatible cli}
    """

    def handle(self):
        pair_file = self.argument('pair_file')
        algo_list = self.argument('algorithms') or ["xdela3 {e} -s {base} {inp} {out}"]
        
       
        files_tested = 0
        size_map = defaultdict(lambda: 0)
        decode_times = defaultdict(lambda: 0)
        encode_times = defaultdict(lambda: 0)
        base_covered = set()
        with tempfile.TemporaryDirectory() as tmpdirname:
            with open(pair_file, 'r') as f:
                for pair in f.readlines():
                    files_tested += 1
                    base, target = pair.split(':')
                    if base not in base_covered:
                        base_size = os.path.getsize(base) 
                        size_map['original'] += base_size
                        for algo in algo_list:
                            size_map[algo] += base_size

                    size_map['original'] += os.path.getsize(target)
                    for algo in algo_list:
                        with TimeToDict(encode_times, algo):
                            cmd = algo.format_map(dict(e='-e', base=base, inp=target, out=f'{tmpdirname}/out.delta'))
                            os.system(cmd)
                        with TimeToDict(decode_times, algo):
                            cmd = algo.format_map(dict(e='-d', base=base, inp=f'{tmpdirname}/out.delta', out=f'{tmpdirname}/out.target'))
                            os.system(cmd)

                        # Verify files & add size to counter
                        filecmp.clear_cache()
                        assert filecmp.cmp(f'{tmpdirname}/out.target', target), f"File decoded from delta is different from the original: algo={algo}, base={base}, target={target}"
                        size_map[algo] += os.path.getsize(f'{tmpdirname}/out.delta')

        for algo in algo_list:
            decode_times[algo] /= files_tested
            encode_times[algo] /= files_tested

        print(f"Algorithm\t\t| Decode(s)\t\t | Encode(s)\t\t | Size(MB)\t\t")
        for algo in size_map:
            print(f"{algo}\t\t| {decode_times.get(algo)}\t\t | {encode_times.get(algo)}\t\t | {size_map.get(algo)//1e6}")
            
class GeneratePairsCommand(Command):
    """
    Generates pairs for base & delta files by detecting similar file. Text pairs are output in format: '{base}:{target}'

    generate_pairs 
        {directory : Directory containing similar files}
        {--o|output=/dev/null : Output file of pairing}
    """

    def handle(self):
        directory = self.argument('directory')
        assert os.path.exists(directory), "Directory should exist"
        
        files = set(glob(f'{directory}/*'))
        si = SimilarityIndex()
        size_map = {} # Store sizes of files so we can sort them by that later

        for file in files:
            # Build up index
            with open(file, 'rb') as f:
                content = f.read()
                size_map[file] = len(content)
                si.add(file, content)

        outf = self.option('output')
        with open(outf, 'w+') as f:
            while len(files):
                file = files.pop()
                similars = list(si.find_similar(file))
                if not similars:
                    continue

                # TODO: can do some different strategies here
                largest = argmax(intersect_dict(size_map, similars))
                for similar in similars:
                    if similar == largest:
                        continue
                    f.write(f'{largest}:{similar}\n')

                files = list(set(files) - set(similars))



application = Application()
application.add(GeneratePairsCommand())

if __name__ == '__main__':
    application.run()
